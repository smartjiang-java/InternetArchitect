
## 01:高并发,负载均衡,高可用
不要因为技术而技术  注意:位运算是计算机计算速度最快的
计算机是软件工程学,强调的是分层解耦 ,OSI分层参考模型

协议是什么?  
1:socket(套接字通信)  tcp建立通信,先从传输控制层开始向下,返回一个连接,才开始应用层
  套接字:localhost:8080   域名+端口号
2:传输数据(http协议:规范标准)

[在浏览器输入一个网址-->DNS解析--->通过TCP协议封装数据--->把TCP的报文封装成IP数据报--->通过ARP找到下一个要发送的机器,
封装成MAC地址--->缝装成帧发送]
每一层都有表 
用户态
[应用层:http,ssh协议  浏览器使用http协议,虚拟机使用ssh协议,发邮件smtp协议]

以下是内核态
[传输控制层:tcp udp :准备握手包 ] 
tcp面向连接(有确认),可靠的 三次握手(双方的输入和输出都保证),通知服务端开辟资源,默认开启一个端口号
四次挥手：一个操作系统的可用端口号为65535个，数量有限，四次分手是为了回收端口号
        1:用户端发送断开请求  2:服务端确认知道分手  3:服务端发送断开请求  4:用户端确认知道
        为什么? 双方都发送并确认,才能断开连接
三次握手--->数据传输-->四次挥手   这个过程称为一个最小粒度,不可被分割

[网络层:ip  路由表,自动生成(route -n) --下一跳机制,每一个互联网的设备只存储自己一步以内的设备 同一个局域网不需要下一条,不需要网关]
一般的网络会有[四个维度:
ip:独特的标识,是端口间的 192.168
掩码(常见255.255.255.0):与ip地址按位与,得出网络号,得到下一跳
网关:
dns域名解析地址:把域名给它,返回ip地址]

[链路层:mac地址(网卡地址 arp -a) ,下一跳给这个地址,每次寻找,改变的是MAC地址,mac是节点间的]
[目标设备的端口号:找到服务进程  目标设备ip地址:找到服务设备  mac地址:找到下一跳] 每次改变的是mac地址

[物理层:把两台计算机连接起来]


对外隐藏，对内可见：将 arp_igore设置为1，app_announce设置为2
现在os内核一般都带有lvs负载均衡模块，模块名称叫做ipvs   需要装一个软件ipvsadm，让这个模块和外界交互

[负载均衡动态负载算法]：
这个负载均衡器均有偷窥能力,当一个数据包(服务端域客户端第一次握手)来到负载均衡器的时候，窥视数据包的目标ip,并记录下来，知道数据包发给谁了;
第二次握手可能无法知道，但是第三次握手的时候，负载均衡器是可以再次偷窥到的，此时给服务器数量加1;断开原理也是如此。

实验手册：
VMware 提供虚拟网络net模式  虚拟的服务器主机  在windows中虚拟出来一块网卡

数据包包着数据包，常见的技术如VPN，翻墙
来的时候经过负载均衡，走的时候直接由服务端返回

一块物理网卡上可以有几个不同的地址

lo本地回环地址 ,这是虚拟网卡地址，内核知道有，外界不知道有，所以需要将目标地址配置到回环地址上。

[逐层布置]：
先配置网络层的vip,:三台虚拟机,一台做负载,另外两台做服务器
1:对于负载机的DIP已经好了,现在配置VIP    ifconfig eth0:2 192.168.150.100/24   注意/24代表255.255.255.0   /16代表255.255.0.0
如何让撤销  ifcinfig  eth0:2 down
2:对于另外的两台的服务器,需要先修改内核协议,防止通告,再去配置VIP(每台服务器均需要这样配置)
修改协议
cd /proc/sys/net/ipv4/conf/eth0
ll
echo 1 > arp_ignore  进行修改,使用vim无法修改
cat arp_ignore    确认
echo 2 > arp_announce
cat arp_announce
为了保证其他接口也生效,需要全局配置
cd ..
cd all
echo 1 > arp_ignore
echo 2 > arp_announce
配置VIP
ifconfig l0:2  192.168.150.100  netmask 255.255.255.255
注意:这个掩码为什么配置为255.255.255.255,而不是255.255.255.0  
防止地址与掩码与运算之后重复,两个地址都可以去,但是物理网卡与内核虚拟网卡比较,虚拟网卡更近,又因为环回接口又会把请求发送给自己,导致数据包发不出去
3:RS中的服务(服务机器)
yum install httpd -y  安装    
service httpd start    启动
vi   /var/www/html/index/html   创建一个主页    ,写入内容  from 192.168.150.12     注意:从浏览器访问192.168.150.13:80
4:配置ipvsadm软件和ipvs模块交互:负载机
yum install ipvsadm
ipvsadm -A -t 192.168.150.100:80  -s   rr  基于进来的数据包
ipvsadm -a -t 192.168.150.100:80  -r  192.168.1501.12(服务机地址) -g -w 1 将数据包负载
ipvsadm -ln 查看配置的规则
ipvsadm -a -t 192.168.150.100:80  -r  192.168.1501.13 -g -w 1  有几台服务器,就写几次
5:验证浏览器访问 192.168.150.100 看到负载,疯狂F5
  node -01
           netstat -natp  结论负载机看不到socket连接,服务机能看到socket连接
6:查看负载的头盔记录表
ipvsadm -lnc      state是FIN_WAIT是连接过,偷窥过所有的包   SYN_RECV:说明三次有问题,证明后面的网络层有问题,负载没问题

问题来了：  
1：如果负载均衡机挂掉了怎么办？服务就停止了    负载均衡机单点故障
2：服务机挂掉了，会造成部分客户端业务不正常?   剔除有故障的服务机
解决方法：
1：单点故障：一变多，多个负载机    
  2个思路： 多点： 主备（多使用这个，VIP不能同时出现，主挂了备机才可以出现）    主主     注意：主（会对主做主备）从
   问题来了：1:方向性->备机如何知道主机挂了？   a:备机定时轮询查看    b：主机周期性向备机发送广播（有重试机制，一般是三次）
           2:效率性->哪一个备机去代替？     给备机加上权重，退让制，没有附加条件的话会争抢
2：服务器挂掉：
 问题来了：
  1：如何确定一个服务机挂掉？    最简单的方式:验证应用层的http协议 ，发请求，判断返回200 oK 

企业追求的是自动化：可以修改ipvs源码 ，也可以写第三方软件，现成的keepalived,代替人自动运维，解决单点故障，实现HA
1：监控自己服务
2：MAster通告自己还活着，Backup监听Master状态，Master挂了，一推Backup推举出来一个，主备都要装keepalived
3:配置vip，添加ipvs配置，引出了keepalived是有配置文件的
4：对后端服务机做健康检查
结论：keepalived是一个通用的工具，主要作为HA实现
    Ngiex，可以作为公司的负载均衡来用，ngiex成为了单点故障，也可以用keepalived来实现
但是谁都有自己的缺点：主机异常退出，但是忘记收回VIP了，内核配置也没有回收，会导致备机的网卡配置VIP，因此主备机都配置VIP，
                  肯呢个会导致客户端的三次握手和四次挥手可能会负载到两台机器上，破坏原子性，导致数据包混乱，或者丢包


## Redis
### 1：什么是Redis?
 Redis是基于内存的，速度比较快，基于键值对（K，V）的，工作线程worker是单线程的，IO操作是多线程的，连接很多，连接池比较大,epoll(NIO)。
 k,v有五种类型，有本地方法，可以很好的实现计算向数据移动，IO优化。
 整体模型是串行化/原子:并行vs串行

### 2：memcached和redis对比的优势？
--memcached也是k,v的，也是做缓存的，value只能是string，也没有本地方法，取数据的时候只能取全部的数据，把字符串转成数组，再去根据下标取
  redis在取数据的时候不需要取全局数据，计算过程在redis内部
1：多个请求来临的时候：epoll多路复用器的作用是：并不是读数据，而是告诉一个事件，知道有哪些可以读
2：redis进行IO读取，单线程串行化，比数据库串行化比较快，数据库保证同步，要不是加锁，要不是使用事务，比redis多了这些操作
   弊端：服务端的另外的核心cpu岂不是浪费了？
在同一个IO里读到的是有序的
   ==》传统：c2 read，计算，写回   ;  c1 read, 计算， 写回  6.0以前
       现在： worker线程做计算，其他线程进行读取和写回     6.0以后,不是默认开启的，需要开启
3：计算，也是串行化


























































 





           









